# Quick HPO configuration for testing

hpo:
  # Study configuration
  study:
    study_name: "deberta-v3-hpo-quick"
    storage: "sqlite:///optuna.db"
    direction: "maximize"
    load_if_exists: true

  # Trial configuration - reduced for quick testing
  trials:
    n_trials: 10  # Fewer trials for quick testing
    timeout: null
    n_jobs: 1

  # Pruning configuration
  pruner:
    type: "median"
    n_startup_trials: 3  # Reduced
    n_warmup_steps: 0
    interval_steps: 1

  # Sampler configuration
  sampler:
    type: "tpe"
    n_startup_trials: 5  # Reduced

  # Search space - same as default
  search_space:
    learning_rate:
      type: "log_uniform"
      low: 1.0e-6
      high: 1.0e-4

    per_device_train_batch_size:
      type: "categorical"
      choices: [8, 16, 32]

    num_train_epochs:
      type: "int"
      low: 2
      high: 5

    warmup_ratio:
      type: "uniform"
      low: 0.0
      high: 0.1

    weight_decay:
      type: "uniform"
      low: 0.0
      high: 0.05

    loss_function:
      type: "categorical"
      choices: ["weighted_ce", "focal"]

    focal_gamma:
      type: "uniform"
      low: 1.0
      high: 3.0

    focal_alpha:
      type: "uniform"
      low: 0.1
      high: 0.5

  # Metric to optimize
  metric:
    name: "macro_f1"
    fold_aggregation: "mean"

  # MLflow integration
  mlflow:
    experiment_name: "deberta-v3-hpo-quick"
    nested_runs: true
    log_params: true
    log_metrics: true
    log_artifacts: true
